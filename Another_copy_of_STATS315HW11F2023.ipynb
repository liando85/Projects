{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liando85/Projects/blob/main/Another_copy_of_STATS315HW11F2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n",
        "\n",
        "To copy the notebook, go to File and click create \"Save a copy to ...\" and work on that copy.\n",
        "\n",
        "Please submit a pdf of the page of your notebook (Ctrl + p on the page, save as pdf, and submit that pdf) on gradescope.\n",
        "\n",
        "Please remember to assign pages to the appropriate questions. Not doing so will result in the deduction of points. Please submit a **pdf** version of the colab notebook.\n",
        "\n",
        "We will not rerun your uploaded notebook, so make sure to run each cell before downloading so that all outputs and plots are visible on the saved pdf.\n"
      ],
      "metadata": {
        "id": "-16kloZ0RHfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0 Part 1\n",
        "Do you have confusions or questions about the previous lectures?  (This is optional to answer)"
      ],
      "metadata": {
        "id": "K8OBW1HTLPh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Answer here)"
      ],
      "metadata": {
        "id": "93l6Bzn8LURO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0 Part 2\n",
        "Any suggestions or thoughts about the course? (This is optional to answer)"
      ],
      "metadata": {
        "id": "OpE9eLm7LSLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Answer here)"
      ],
      "metadata": {
        "id": "BKl2S-7OLVHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNNs and NLP\n",
        "We now want to extend the core deep learning you have learned in the course so far. Deep learning as an applied field is evolving into a number of branches, such as computer vision and NLP. As the number of use cases grows, we will likely see even greater divergence going forward, with each specialization having its own bespoke set of layers that are widely used. If you are interested in pushing the frontiers of this field further, the development of such new layers is of great importance. Here, we'll be discussing another such specialization to help give you an idea of both how deep learning is currently used, but hopefully, even more importantly, how it is that you can extend this framework going forward."
      ],
      "metadata": {
        "id": "ag9w8ExZRY6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential and Time Series Data Analysis\n",
        "\n",
        "In many cases, we would like to make predictions on sequential, time series data. For example, perhaps we would like to forecast the weather for tomorrow, maybe we want to predict the price of a particular stock 5 days from now, or maybe we want to generate a caption for an image. For these tasks, our models need to deal with inputs and outputs that could have *variable* sequence lengths. This is in sharp contrast to the classification and regression tasks we have previously been working on, where the model (i.e. CNN) can only process a single input of fixed dimension and output a single response of fixed dimension. In this homework, we will explore neural network architecures for tasks where both inputs and outputs may be of variable length. In particular, we will study **Recurrent Neural Networks (RNN)**, a powerful deep learning archietcture for sequential and time series data analysis tasks."
      ],
      "metadata": {
        "id": "5bpqbFEjLOcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Network"
      ],
      "metadata": {
        "id": "rgvNvX0RO_Vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recurrent Neural Network (RNN) is a blackbox (Left of Figure 2) with it an “internal state” that is updated as a sequence is processed. At every single timestep, we feed in an input vector into RNN where it modifies that state as a function of what it receives. When we tune RNN weights, RNN will show different behaviors in terms of how its state evolves as it receives these inputs. We are also interested in producing an output based on the RNN state, so we can produce these output vectors on top of the RNN (as depicted below).\n",
        "\n",
        "If we unroll an RNN model (Right of Figure 2), then there are inputs (e.g. video frame) at different timesteps shown as $x_1, x_2, x_3, ..., x_t$. RNN at each timestep takes in two inputs – an input frame $(xi)$ and previous representation of what it seems so far (i.e. history $h_{t-1}$) – to generate an output $\\hat{y}_t$ and update its history, which will get forward propagated over time. All the RNN blocks in the figure below are the same block that share the same parameter, but have different inputs and history at each timestep.\n",
        "\n",
        "![](https://cs231n.github.io/assets/rnn/unrolledRNN.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jz3UL6WJPoXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More precisely, the RNN can be represented by a sequence of hidden states $h_1, ..., h_t$, where each hidden state $h_t$ can be computed as some function of the previous hidden state $h_{t-1}$, the current input $x_t$, a set of weights $W$:\n",
        "\n",
        "$$h_t = f_W(h_{t-1}, x_t)$$\n",
        "\n",
        "A fixed function $f_W$ with weights $W$ is applied at every single timestep and that allows us to use the Recurrent Neural Network on sequences without having to commit to the size of the sequence because we apply the exact same function at every single timestep, no matter how long the input or output sequences are.\n",
        "\n",
        "In the most simplest form of RNN, which we call a Vanilla RNN, the network is just a single hidden state $h$ where we use a recurrence formula that basically tells us how we should update our hidden state $h$ as a function of the previous hidden state $h_{t-1}$ and the current input $x_t$. Specifically, in RNNs the following parametric method is used to update the hidden state from time $t-1$ to time $t$:\n",
        "\n",
        "$$h_t = tanh(W_{h}h_{t-1} + W_{x}x_t + b_h)$$\n",
        "\n",
        "Here, $W_{h}$ and $W_{x}$ are fixed weight matrices (that do not depend on $t$), $b_h$ is a bias vector, and $tanh$ is the hyperbolic tan function. The figure below provides a nice visualization of this recurrence.\n",
        "\n",
        "![](https://cs231n.github.io/assets/rnn/vanilla_rnn_mformula_1.png)\n",
        "\n",
        "The hidden state $h_t$  captures information about all previous inputs $x_1, ... x_{t-1}$  and fully parameterizes the RNN at time point $t$ in the input sequence . A prediction at time point $t$, $\\hat{y}_t$, can thus be computed exclusively using $h_t$,  another weight matrix $W_{o}$, and another bias $b_o$ as follows:\n",
        "\n",
        "$$\\hat{y}_t = W_{o}h_t + b_o$$\n",
        "\n",
        "\n",
        "![](https://cs231n.github.io/assets/rnn/vanilla_rnn_mformula_2.png)\n",
        "\n",
        "A few comments about $W_{h}$, $W_{x}$, $W_{o}$, $b_h$, and $b_o$. Note that these weights are not subscripted by $t$. This means that these weight are *shared* across all timesteps of the input sequence. These weights are also the trainable parameters that need to be learnt and updated during gradient descent and backpropagation. We typically represent the dimension of the hidden state (which is a vector) by $H$, the dimension of a single input sequence of size $T$ as $(1, T, D)$, and the dimension of the output as $Q$. Then, $W_h \\in \\mathbb{R}^{HxH}$, $W_x \\in \\mathbb{R}^{HxD}$, $W_o \\in \\mathbb{R}^{QxH}$, $b_h \\in \\mathbb{R}^H$, and $b_o \\in \\mathbb{R}^Q$."
      ],
      "metadata": {
        "id": "bJJmvEbLP_Hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, RNNs allow us to wire up an architecture, where the prediction at every single timestep is a function of all the timesteps that have come before. The recurrence based nature of their architecture makes them extremely adaptable to various types of sequential tasks, a few of which have been shown below.\n",
        "\n",
        "![](https://cs231n.github.io/assets/rnn/types.png)\n",
        "\n",
        "Here,  red boxes are input vectors, green boxes are hidden layers, blue boxes are output vectors. Note that traditional classification algorithms like CNN can be classified as a one-to-one RNN. Thus, RNNs are a genealization of the models we have been working with so far in this course! For a more in-depth introduction to RNNs, we recommend you check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) excellent blog post.\n",
        "\n",
        "In this question you will be tasked with implementing many-to-one and many-to-many RNN architectures from **scratch** with the help of Tensorflow's autograd feature to compute gradients automatically. That said, just like in Homework 2, we highly recommend that you re-derive the gradients for all trainable weights to solidify your understanding of the architecture."
      ],
      "metadata": {
        "id": "8WnroddHTk7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Let's start by importing just the vanilla packages as usual:"
      ],
      "metadata": {
        "id": "hNKznzaQiJ2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib.request\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YYqXKntyiUqp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla RNN [47 pts]"
      ],
      "metadata": {
        "id": "DtZYIn9HLJQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1** [10 pts]"
      ],
      "metadata": {
        "id": "mUnHuTNFNVR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the figures and equations above, complete the function below that computes a single forward step for a Vanilla RNN. That is, given the previous hidden state $h_{t-1}$ and the current input $x_t$ at timestep $t$, compute the next hidden state $h_t$. Note that the input at timestep $t$, $x_t$, could be a vector of size $D$! Pay attention to the dimensions of each input variable."
      ],
      "metadata": {
        "id": "fQHyvGWVXMUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_step_forward(x, prev_h, Wx, Wh, bh):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
        "    activation function.\n",
        "\n",
        "    The input data has dimension D, the hidden state has dimension H, and we use\n",
        "    a minibatch size of N.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for this timestep, of shape (N, D).\n",
        "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases, of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - next_h: Next hidden state, of shape (N, H)\n",
        "    \"\"\"\n",
        "    next_h = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #\n",
        "    # hidden state in the next_h variable. Use the recurrence shown above [3 pts]#                                                                                                 #\n",
        "    ##############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    forward =  tf.matmul(prev_h, Wh) + tf.matmul(x, Wx) + bh\n",
        "    next_h = tf.tanh(forward)\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return next_h"
      ],
      "metadata": {
        "id": "ku0WsdlkfRMq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run* the following to check your implementation. You should see errors on the order of 1e-10 or less."
      ],
      "metadata": {
        "id": "SmDQ9mnyHtLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D, H = 3, 10, 4\n",
        "\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.7, N*D)), [N, D])\n",
        "prev_h = tf.reshape(tf.Variable(tf.linspace(-0.2, 0.5, N*H)), [N, H])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-0.1, 0.9, D*H)), [D, H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.3, 0.7, H*H)), [H, H])\n",
        "bh = tf.Variable(tf.linspace(-0.2, 0.4, H))\n",
        "\n",
        "next_h = rnn_step_forward(x, prev_h, Wx, Wh, bh)\n",
        "expected_next_h = tf.Variable([\n",
        "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
        "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
        "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
        "\n",
        "mse = tf.reduce_sum((next_h - expected_next_h)**2)\n",
        "print('next_h error: ', mse)"
      ],
      "metadata": {
        "id": "KVEBqCAIgsik",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "76eebcfa-c5ed-41e0-97ce-0fc8de4ef1aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_h error:  tf.Tensor(3.996803e-14, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2** [11 pts]"
      ],
      "metadata": {
        "id": "vW2o5k_lNznM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented the forward pass for a single timestep of a vanilla RNN, you will implement a RNN that processes an entire sequence of data. Implement the forward pass through all the timesteps by making calls to the `rnn_step_forward` function that you defined earlier. Your implementation should return both the list of hidden states for all input sequences and also just the last hidden state for all input sequences. Note that `rnn_forward` takes in as input a *batch* of sequences, each of length $T$. Pay attention to the dimensions of each input variable."
      ],
      "metadata": {
        "id": "eWuRDs4jHz0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_forward(x, Wx, Wh, bh):\n",
        "    \"\"\"\n",
        "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
        "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
        "    size of H, and we work over a minibatch containing N sequences. After running\n",
        "    the RNN forward, we return the hidden states for all timesteps.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
        "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
        "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
        "    - b: Biases, of shape (H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
        "    - last_h: Very last hidden state of shape (N, H)\n",
        "    \"\"\"\n",
        "    h, last_h= None, None\n",
        "    (N, T, D) = x.shape #pay attention to the dimension variables here\n",
        "    (H, H) = Wh.shape #pay attention to dimension variables here\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #\n",
        "    # input data. You should use the rnn_step_forward function that you defined  #\n",
        "    # above. You can use a for loop to help compute the forward pass. Make sure  #\n",
        "    # to pay attention to what this function should return above. You need to    #\n",
        "    # store all hidden states in the variable h and the very last hidden         #\n",
        "    # state in the variable last_h. Initialize the hidden state for all input    #\n",
        "    # sequences to all zeros. Hint: you will need to change the shape of x in    #\n",
        "    # order to properly loop over the timestamps. That is, change the shape of x #\n",
        "    # to be (T, N, D) and then loop over timesteps. To compute h, append each    #\n",
        "    # hidden state to a vector and then reshape that vector using tf.tranpose to #\n",
        "    # the desired shape. [4pts]                                                  #\n",
        "    ##############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    h0 = tf.zeros((N, H))\n",
        "    x = tf.transpose(x, perm=[1, 0, 2])\n",
        "    hidden_list = []\n",
        "\n",
        "    last_h = tf.zeros((N, H))\n",
        "    for t in range(T):\n",
        "       last_h = rnn_step_forward(x[t], last_h, Wx, Wh, bh)\n",
        "       hidden_list.append(last_h)\n",
        "\n",
        "    h = tf.transpose(tf.stack(hidden_list), perm=[1, 0, 2])\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return h, last_h\n"
      ],
      "metadata": {
        "id": "PB7QgbOAilTR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following to check your implementation. You should see errors on the order of `1e-10` or less."
      ],
      "metadata": {
        "id": "7G_7Kgs2H_84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, D, H = 1, 3, 4, 5\n",
        "\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.1, 0.3, N*T*D)), [N, T, D])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-0.2, 0.4, D*H)), [D, H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.1, H*H)), [H, H])\n",
        "bh = tf.Variable(tf.linspace(-0.7, 0.1, H))\n",
        "h, last_h = rnn_forward(x, Wx, Wh, bh)\n",
        "expected_h = tf.Variable([[[-0.5902114,  -0.4492275,  -0.28165027, -0.09492856,  0.09872047],\n",
        "  [-0.2199841 , -0.03843261 , 0.14569218 , 0.32024875 , 0.47546807],\n",
        "  [-0.52540594, -0.32617706, -0.09304047 , 0.15076531 , 0.37751395]]])\n",
        "\n",
        "mse = tf.reduce_sum((h - expected_h)**2)\n",
        "print('h error: ', mse)"
      ],
      "metadata": {
        "id": "j78z1wOkkYNj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7f17b486-53dc-47cf-a802-feb4c8eb607a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h error:  tf.Tensor(1.8499091e-14, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3** [11 pts]"
      ],
      "metadata": {
        "id": "K1lJAH3GOEv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, complete the function `make_predictions` to compute the prediction(s) $\\hat{y}_t$ for each input sequence. Note that the number of predictions to be made depends on whether you want a many-to-one or many-to-many RNN architecture. Your implementation should work for either architecure. That is, the input variable `h` can be either a tensor with all the hidden states (one per timestep), or just the last one. Note that since we kept track of the hidden states as we processed each input sequence, we can compute all prediction(s) at the very end. That is, we didn't need to make a prediction $\\hat{y}_t$ as soon as we process the input $x_t$."
      ],
      "metadata": {
        "id": "kmxkQe1HIA82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(h, Wo, bo):\n",
        "    \"\"\"\n",
        "    Compute the prediction(s).\n",
        "\n",
        "    Inputs:\n",
        "    - h: Either the set of all hidden states for all input examples (NxTxH), or just the last one (NxH)\n",
        "    - Wo: Weight matrix for hidden-to-output connections, of shape (HxQ)\n",
        "    - bo: Biases, of shape (Q,)\n",
        "\n",
        "    Returns:\n",
        "    - y: either a sequence of outputs for each example (NxTxQ) or a single output for each example (NxQ)\n",
        "    \"\"\"\n",
        "    y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Compute the predictions below. This should be very simple and can be done\n",
        "    #       in a single line without checking whether h contains the hidden states for all\n",
        "    #       timesteps or just the last one. [1 pts]\n",
        "    ##############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    y = tf.matmul(h, Wo) + bo\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return y"
      ],
      "metadata": {
        "id": "VEQD7ywwrW00"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-10` or less."
      ],
      "metadata": {
        "id": "fAzkTZs3ibBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "Q = 2\n",
        "\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.1, 0.3, N*T*D)), [N, T, D])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-0.2, 0.4, D*H)), [D, H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.1, H*H)), [H, H])\n",
        "Wo = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.1, H*Q)), [H, Q])\n",
        "bh = tf.Variable(tf.linspace(-0.7, 0.1, H))\n",
        "bo = tf.Variable(tf.linspace(-0.7, 0.1, Q))\n",
        "h, last_h = rnn_forward(x, Wx, Wh, bh)\n",
        "\n",
        "y = make_predictions(h, Wo, bo)\n",
        "\n",
        "expected_y = tf.Variable([[[-0.25844076 , 0.4618572 ],\n",
        "  [-0.6047536  , 0.22198313],\n",
        "  [-0.34340593 , 0.41551694]],\n",
        "\n",
        " [[-0.29477337 , 0.44697136],\n",
        "  [-0.6169102  , 0.22335273],\n",
        "  [-0.3735     , 0.40507644]]])\n",
        "\n",
        "mse = tf.reduce_sum((y - expected_y)**2)\n",
        "print('y error: ', mse)"
      ],
      "metadata": {
        "id": "hWYx-JHAtfJn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e79c6e4e-b35a-4a35-a5df-6b4c2b7e0da5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y error:  tf.Tensor(1.2434498e-14, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4** [15 pts]"
      ],
      "metadata": {
        "id": "u4KVUMliOTsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to wrap everything up into a single Keras Model class just like we have been doing in the past homeworks. Complete VanillaRNN Keras Model class below. Note that $D$ is the size of feature vector for a given timestep, $H$ is the size of the hidden state, and $Q$ is the size of the output."
      ],
      "metadata": {
        "id": "L4FdKnuacowP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class VanillaRNN(Model):\n",
        "  def __init__(self, D, H, Q):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "    ##############################################################################\n",
        "    # TODO:  Complete the __init__ function of this Keras Model by creating      #\n",
        "    #        variables for and initializing all the trainable weights of a       #\n",
        "    #        Vanilla RNN. Initialize the trainable weights using                 #\n",
        "    #        a standard normal distribution [1 pts]                              #\n",
        "    ##############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    self.Wh = tf.Variable(tf.random.normal(shape = (H, H)))\n",
        "    self.Wx = tf.Variable(tf.random.normal(shape = (D, H)))\n",
        "    self.Wo = tf.Variable(tf.random.normal(shape = (H, Q)))\n",
        "    self.bh = tf.Variable(tf.zeros(shape = (H, )))\n",
        "    self.bo = tf.Variable(tf.zeros(shape = (Q, )))\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def call(self, x):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the call function of the Keras Model that computes the outputs\n",
        "    #       of the Vanilla RNN for a batch of input sequences. Hint: you'll need to\n",
        "    #.      to use the rnn_forward and the make_predictions functions. Use only the last\n",
        "    #.      hidden state to make predictions for each input sequence. That is, you\n",
        "    #.      are only going to make a single prediction per input sequence, thus your call\n",
        "    #.      function should return a NxQ matrix when given as input a batch of sequences\n",
        "    #       of size NxTxD. [2 pt]\n",
        "    ##############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    h, last_h = rnn_forward(x, self.Wx, self.Wh, self.bh)\n",
        "    y = make_predictions(last_h, self.Wo, self.bo)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "1kxrrmh0wago"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the cell below by instantiating your model and implementing the training loop. Use the training data store in `data` and the associated labels in `labels` to train your model. Make sure to set the loss to Mean Squared Error, the Optimizer to Adam, and to keep track of the training loss. You can assume that we are only using one batch with size $N$ (i.e. our entire training dataset is one batch)"
      ],
      "metadata": {
        "id": "NQcyKk2zdxHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Synthetic Data\n",
        "N, T, D, H, Q = 1000, 5, 2, 32, 1\n",
        "data = tf.reshape(tf.Variable(tf.linspace(-0.4, 1.2, N*T*D)), [N, T, D])\n",
        "labels = np.random.multivariate_normal(np.zeros(Q), np.identity(Q), N)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Make an instance of the VanillaRNN Model and write the training loop #\n",
        "# to train the model. Make sure to use the Mean Squared Error loss, the Adam #\n",
        "# optimizer and to keep track of the training loss.  Make sure you pay       #\n",
        "# attention to where the data and its labels are stored. Use 100 epochs.     #\n",
        "# Hint: you can copy and paste the training loop from previous homeworks.    #\n",
        "# Treat the variable \"data\" as one single batch of data of size N. That is,  #\n",
        "# you should pass in the entire dataset into your training step for each epoch\n",
        "#[3 pts]                                                                     #\n",
        "##############################################################################\n",
        "# Replace \"________\" with your code\n",
        "van_rnn = VanillaRNN(D, H, Q)\n",
        "\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "def train_step(data, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = van_rnn(data)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, van_rnn.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, van_rnn.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "\n",
        "train_step_tf = tf.function(train_step)\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  train_step_tf(data, labels)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "   )\n",
        "##############################################################################\n",
        "#                               END OF YOUR CODE                             #\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "ubp-zIV0LgZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "aa6d0146-c6e6-4e65-8924-a527a5e2aafa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 28.41737937927246, \n",
            "Epoch 2, Loss: 23.56018829345703, \n",
            "Epoch 3, Loss: 17.983625411987305, \n",
            "Epoch 4, Loss: 12.979154586791992, \n",
            "Epoch 5, Loss: 9.577058792114258, \n",
            "Epoch 6, Loss: 7.812780857086182, \n",
            "Epoch 7, Loss: 7.052362442016602, \n",
            "Epoch 8, Loss: 6.566720485687256, \n",
            "Epoch 9, Loss: 6.186821460723877, \n",
            "Epoch 10, Loss: 5.896361351013184, \n",
            "Epoch 11, Loss: 5.689214706420898, \n",
            "Epoch 12, Loss: 5.5410966873168945, \n",
            "Epoch 13, Loss: 5.395874977111816, \n",
            "Epoch 14, Loss: 5.213361740112305, \n",
            "Epoch 15, Loss: 4.98795747756958, \n",
            "Epoch 16, Loss: 4.733733177185059, \n",
            "Epoch 17, Loss: 4.470710754394531, \n",
            "Epoch 18, Loss: 4.215266704559326, \n",
            "Epoch 19, Loss: 3.9785165786743164, \n",
            "Epoch 20, Loss: 3.7685203552246094, \n",
            "Epoch 21, Loss: 3.58949613571167, \n",
            "Epoch 22, Loss: 3.442040205001831, \n",
            "Epoch 23, Loss: 3.3178067207336426, \n",
            "Epoch 24, Loss: 3.2049155235290527, \n",
            "Epoch 25, Loss: 3.0954298973083496, \n",
            "Epoch 26, Loss: 2.9856841564178467, \n",
            "Epoch 27, Loss: 2.8749358654022217, \n",
            "Epoch 28, Loss: 2.764533519744873, \n",
            "Epoch 29, Loss: 2.6566860675811768, \n",
            "Epoch 30, Loss: 2.5530762672424316, \n",
            "Epoch 31, Loss: 2.4545254707336426, \n",
            "Epoch 32, Loss: 2.3615827560424805, \n",
            "Epoch 33, Loss: 2.2748911380767822, \n",
            "Epoch 34, Loss: 2.1950342655181885, \n",
            "Epoch 35, Loss: 2.122199296951294, \n",
            "Epoch 36, Loss: 2.05601167678833, \n",
            "Epoch 37, Loss: 1.9956600666046143, \n",
            "Epoch 38, Loss: 1.9401352405548096, \n",
            "Epoch 39, Loss: 1.8884671926498413, \n",
            "Epoch 40, Loss: 1.8398876190185547, \n",
            "Epoch 41, Loss: 1.7938339710235596, \n",
            "Epoch 42, Loss: 1.7497795820236206, \n",
            "Epoch 43, Loss: 1.7070568799972534, \n",
            "Epoch 44, Loss: 1.664973497390747, \n",
            "Epoch 45, Loss: 1.6231635808944702, \n",
            "Epoch 46, Loss: 1.5817887783050537, \n",
            "Epoch 47, Loss: 1.5414091348648071, \n",
            "Epoch 48, Loss: 1.5026901960372925, \n",
            "Epoch 49, Loss: 1.4661470651626587, \n",
            "Epoch 50, Loss: 1.4320114850997925, \n",
            "Epoch 51, Loss: 1.400198221206665, \n",
            "Epoch 52, Loss: 1.3703545331954956, \n",
            "Epoch 53, Loss: 1.3419888019561768, \n",
            "Epoch 54, Loss: 1.3146592378616333, \n",
            "Epoch 55, Loss: 1.2881460189819336, \n",
            "Epoch 56, Loss: 1.2625547647476196, \n",
            "Epoch 57, Loss: 1.2383074760437012, \n",
            "Epoch 58, Loss: 1.216040015220642, \n",
            "Epoch 59, Loss: 1.1964313983917236, \n",
            "Epoch 60, Loss: 1.1799958944320679, \n",
            "Epoch 61, Loss: 1.1669038534164429, \n",
            "Epoch 62, Loss: 1.156890869140625, \n",
            "Epoch 63, Loss: 1.1493415832519531, \n",
            "Epoch 64, Loss: 1.1435234546661377, \n",
            "Epoch 65, Loss: 1.1388179063796997, \n",
            "Epoch 66, Loss: 1.1348108053207397, \n",
            "Epoch 67, Loss: 1.131245732307434, \n",
            "Epoch 68, Loss: 1.1279621124267578, \n",
            "Epoch 69, Loss: 1.124869704246521, \n",
            "Epoch 70, Loss: 1.1219236850738525, \n",
            "Epoch 71, Loss: 1.1190792322158813, \n",
            "Epoch 72, Loss: 1.1162407398223877, \n",
            "Epoch 73, Loss: 1.1132397651672363, \n",
            "Epoch 74, Loss: 1.1098823547363281, \n",
            "Epoch 75, Loss: 1.1060315370559692, \n",
            "Epoch 76, Loss: 1.101683497428894, \n",
            "Epoch 77, Loss: 1.096979022026062, \n",
            "Epoch 78, Loss: 1.0921485424041748, \n",
            "Epoch 79, Loss: 1.0874264240264893, \n",
            "Epoch 80, Loss: 1.0829800367355347, \n",
            "Epoch 81, Loss: 1.0788778066635132, \n",
            "Epoch 82, Loss: 1.0751043558120728, \n",
            "Epoch 83, Loss: 1.0715925693511963, \n",
            "Epoch 84, Loss: 1.0682686567306519, \n",
            "Epoch 85, Loss: 1.0650886297225952, \n",
            "Epoch 86, Loss: 1.0620492696762085, \n",
            "Epoch 87, Loss: 1.0591868162155151, \n",
            "Epoch 88, Loss: 1.0565534830093384, \n",
            "Epoch 89, Loss: 1.0541906356811523, \n",
            "Epoch 90, Loss: 1.052106499671936, \n",
            "Epoch 91, Loss: 1.0502656698226929, \n",
            "Epoch 92, Loss: 1.0485996007919312, \n",
            "Epoch 93, Loss: 1.0470317602157593, \n",
            "Epoch 94, Loss: 1.0454998016357422, \n",
            "Epoch 95, Loss: 1.0439705848693848, \n",
            "Epoch 96, Loss: 1.0424400568008423, \n",
            "Epoch 97, Loss: 1.0409231185913086, \n",
            "Epoch 98, Loss: 1.0394353866577148, \n",
            "Epoch 99, Loss: 1.037986397743225, \n",
            "Epoch 100, Loss: 1.0365750789642334, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If implemented correctly, you should see a final loss close to $1$."
      ],
      "metadata": {
        "id": "RPQ2xT97e7Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM [27 pts]"
      ],
      "metadata": {
        "id": "-QJgsSTbMO7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have seen only a simple recurrence formula for the Vanilla RNN. In practice, we actually will rarely ever use Vanilla RNN formula. Instead, we will use what we call a Long-Short Term Memory (LSTM) RNN.  Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows. To learn more about LSTMs are their intuition, we again refer you to [this](https://http://colah.github.io/posts/2015-08-Understanding-LSTMs/) article.\n",
        "\n",
        "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
        "\n",
        "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "i = \\sigma(a_i) \\hspace{2pc}\n",
        "f = \\sigma(a_f) \\hspace{2pc}\n",
        "o = \\sigma(a_o) \\hspace{2pc}\n",
        "g = \\tanh(a_g)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
        "\n",
        "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
        "\n",
        "$$\n",
        "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
        "h_t = o\\odot\\tanh(c_t)\n",
        "$$\n",
        "\n",
        "where $\\odot$ is the elementwise product of vectors.\n",
        "\n",
        "In the rest of the notebook we will implement the LSTM update rule.\n",
        "\n",
        "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$, and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$, $b \\in \\mathbb{R}^{4H}$  so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h + b$"
      ],
      "metadata": {
        "id": "yEw9B6gDfaDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like the Vanilla RNN, predictions of a LSTM can be computed as:\n",
        "\n",
        "$$\\hat{y}_t = W_oh_t + b_o$$\n",
        "\n",
        "where $W_o \\in \\mathbb{R}^{QxH}$ and $b_o \\in \\mathbb{R}^{Q}$ are again additional trainable weights.\n"
      ],
      "metadata": {
        "id": "j6DXzC6JG5HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the forward pass for a single timestep of an LSTM in the `lstm_step_forward` function. This should be similar to the `rnn_step_forward` function that you implemented above, but using the LSTM update rule instead."
      ],
      "metadata": {
        "id": "bIRj1xfagJ80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q5** [7]"
      ],
      "metadata": {
        "id": "EyEer6SHPCbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, bh):\n",
        "    \"\"\"\n",
        "    Forward pass for a single timestep of an LSTM.\n",
        "\n",
        "    The input data has dimension D, the hidden state has dimension H, and we use\n",
        "    a minibatch size of N.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, D)\n",
        "    - prev_h: Previous hidden state, of shape (N, H)\n",
        "    - prev_c: previous cell state, of shape (N, H)\n",
        "    - Wx: Input-to-hidden weights, of shape (D, 4H)\n",
        "    - Wh: Hidden-to-hidden weights, of shape (H, 4H)\n",
        "    - b: Biases, of shape (4H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - next_h: Next hidden state, of shape (N, H)\n",
        "    - next_c: Next cell state, of shape (N, H)\n",
        "    \"\"\"\n",
        "    next_h, next_c = None, None\n",
        "    H = prev_h.shape[1]\n",
        "    #############################################################################\n",
        "    # TODO: Implement the forward pass for a single timestep of an LSTM. Pay    #\n",
        "    #       attention to what this function needs to return. [2pts]             #\n",
        "    #############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    forward = tf.matmul(x, Wx) + tf.matmul(prev_h, Wh) + bh\n",
        "\n",
        "    #get components\n",
        "    a_i, a_f, a_o, a_g = tf.split(forward, 4, axis=1)\n",
        "\n",
        "    input_gate = tf.sigmoid(a_i)\n",
        "    forget_gate = tf.sigmoid(a_f)\n",
        "    output_gate = tf.sigmoid(a_o)\n",
        "    block_gate = tf.tanh(a_g)\n",
        "\n",
        "    next_c = forget_gate * prev_c + input_gate * block_gate\n",
        "    next_h = output_gate * tf.tanh(next_c)\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return next_h, next_c"
      ],
      "metadata": {
        "id": "AUw7rCNbM_Pa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-10` or less."
      ],
      "metadata": {
        "id": "XctqtfB3gPWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D, H = 3, 4, 3\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.4, 1.2, N*D)), [N, D])\n",
        "prev_h = tf.reshape(tf.Variable(tf.linspace(-0.3, 0.7, N*H)), [N, H])\n",
        "prev_c = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.9, N*H)), [N, H])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-2.1, 1.3, 4*D*H)), [D, 4 * H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.7, 2.2, 4*H*H)), [H, 4 * H])\n",
        "bh = tf.Variable(tf.linspace(0.3, 0.7, 4*H))\n",
        "\n",
        "next_h, next_c = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, bh)\n",
        "\n",
        "expected_next_h = tf.Variable([[0.2504206, 0.3178399, 0.37452143],\n",
        " [0.39561146, 0.50878483, 0.61119807],\n",
        " [0.34590057, 0.526227 ,  0.6923673 ]])\n",
        "expected_next_c = tf.Variable([[0.3342887 , 0.44137198, 0.5436997 ],\n",
        " [0.55522066, 0.7301363 , 0.914896  ],\n",
        " [0.46848923, 0.71435446, 1.0070218 ]])\n",
        "\n",
        "mse_h = tf.reduce_sum((next_h - expected_next_h)**2)\n",
        "mse_c = tf.reduce_sum((next_c - expected_next_c)**2)\n",
        "\n",
        "print('next_h error: ', mse_h)\n",
        "print('next_c error: ', mse_c)"
      ],
      "metadata": {
        "id": "gJsPsH-qNiQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "339597d3-ffda-489c-f57b-41b405fff0f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_h error:  tf.Tensor(1.8651747e-14, shape=(), dtype=float32)\n",
            "next_c error:  tf.Tensor(3.907985e-14, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like before, now implement the `lstm_forward` function to run an LSTM forward on an entire timeseries of data."
      ],
      "metadata": {
        "id": "VoXqXA3dgUT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q6** [10]"
      ],
      "metadata": {
        "id": "QVVj1DeyPFyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, Wx, Wh, bh):\n",
        "    \"\"\"\n",
        "    Forward pass for an LSTM over an entire sequence of data. We assume an input\n",
        "    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden\n",
        "    size of H, and we work over a minibatch containing N sequences. After running\n",
        "    the LSTM forward, we return the hidden states for all timesteps and the very\n",
        "    last hidden state.\n",
        "\n",
        "    Note that the cell state is not returned; it is\n",
        "    an internal variable to the LSTM and is not accessed from outside.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, T, D)\n",
        "    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)\n",
        "    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)\n",
        "    - b: Biases, of shape (4H,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)\n",
        "    - last_h: Very last hidden state of shape (N, H)\n",
        "    \"\"\"\n",
        "    h, last_h = None, None\n",
        "    (N, T, D) = x.shape\n",
        "    H, _ = Wh.shape\n",
        "    #############################################################################\n",
        "    # TODO: Implement the forward pass for an LSTM over an entire timeseries.   #\n",
        "    # You should use the lstm_step_forward function that you just defined.\n",
        "    # Your code should look very similar to rnn_forward. Initialize the initial\n",
        "    # hidden states and cell states to all zeros. [2pts]\n",
        "    #############################################################################\n",
        "    # Replace \"________\" with your code\n",
        "    h0 = tf.zeros((N, H))\n",
        "    c0 = tf.zeros_like(h0)\n",
        "\n",
        "    prev_h = h0\n",
        "    prev_c0 = c0\n",
        "\n",
        "    x = tf.transpose(x, perm=[1,0,2])\n",
        "    hidden_list = []\n",
        "\n",
        "    for t in range(T):\n",
        "      if t == 0:\n",
        "        prev_h = h0\n",
        "        prev_c = c0\n",
        "      else:\n",
        "        prev_h = last_h\n",
        "        prev_c = c\n",
        "\n",
        "      last_h, c = lstm_step_forward(x[t], prev_h, prev_c, Wx, Wh, bh)\n",
        "      hidden_list.append(last_h)\n",
        "\n",
        "    h = tf.transpose(tf.stack(hidden_list), perm=[1,0,2])\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return h, last_h"
      ],
      "metadata": {
        "id": "A821IwODO5Cj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are done, run the following to check your implementation. You should see an error on the order of `1e-10` or less."
      ],
      "metadata": {
        "id": "qVztDsV5hWqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D, H, T = 1, 5, 4, 3\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.4, 1.2, N*T*D)), [N, T, D])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-2.1, 1.3, 4*D*H)), [D, 4 * H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.7, 2.2, 4*H*H)), [H, 4 * H])\n",
        "bh = tf.Variable(tf.linspace(0.3, 0.7, 4*H))\n",
        "\n",
        "h, last_h= lstm_forward(x, Wx, Wh, bh)\n",
        "\n",
        "expected_h = tf.Variable([[[0.544338, 0.5421794, 0.54000026, 0.53780025],\n",
        "  [0.7619797 , 0.79446   , 0.82151794, 0.8438525 ],\n",
        "  [0.69012046, 0.78758657, 0.8571147 , 0.9037268 ]]])\n",
        "\n",
        "mse = tf.reduce_sum((expected_h - h)**2)\n",
        "\n",
        "print('next_h error: ', mse)\n"
      ],
      "metadata": {
        "id": "o6NMWfb2Rii_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d06dc1f8-d416-43d9-baf3-daa0be57d108"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "next_h error:  tf.Tensor(5.3290705e-14, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q7** [10]"
      ],
      "metadata": {
        "id": "2cJZ3v3ePMy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, wrap everything up into a Keras Model class just like before."
      ],
      "metadata": {
        "id": "qf_h35lAhZOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class LSTMRNN(Model):\n",
        "  ##############################################################################\n",
        "  # TODO: Create and complete both the __init__ and call functions for the LSTMRNN Keras  #\n",
        "  # Model. Just like for the VanillaRNN, use only the last hidden state        #\n",
        "  # to make predictions for each input sequence.  Hint: you can reuse the      #\n",
        "  # make_predictions function. Initialize the trainable weights using          #\n",
        "  # a standard normal distribution  [2pts]                                     #\n",
        "  ##############################################################################\n",
        "  # Replace \"________\" statement with your code\n",
        "  def __init__(self, D, H, Q):\n",
        "    super(LSTMRNN, self).__init__()\n",
        "    self.Wh = tf.Variable(tf.random.normal(shape=(H, 4 * H)))\n",
        "    self.Wx = tf.Variable(tf.random.normal(shape=(D, 4 * H)))\n",
        "    self.Wo = tf.Variable(tf.random.normal(shape=(H, Q)))\n",
        "    self.bh = tf.Variable(tf.zeros(shape=(4 * H,)))\n",
        "    self.bo = tf.Variable(tf.zeros(shape=(Q,)))\n",
        "\n",
        "  def call(self, x):\n",
        "    h, last_h = lstm_forward(x, self.Wx, self.Wh, self.bh)\n",
        "    y = make_predictions(last_h, self.Wo, self.bo)\n",
        "    return y\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n"
      ],
      "metadata": {
        "id": "1JnCWpn5VmFH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the cell below by instantiating your model and implementing the training loop. Use the training data store in `data` and the associated labels in `labels` to train your model. Make sure to set the loss to Mean Squared Error, the Optimizer to Adam, and to keep track of the training loss. You can assume that we are only using one batch with size $N$ (i.e. our entire training dataset is one batch)"
      ],
      "metadata": {
        "id": "2oMZvMKljb7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Synthetic Data\n",
        "N, T, D, H, Q = 1000, 5, 2, 32, 1\n",
        "data = tf.reshape(tf.Variable(tf.linspace(-0.4, 1.2, N*T*D)), [N, T, D])\n",
        "labels = np.random.multivariate_normal(np.zeros(Q), np.identity(Q), N)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Make an instance of the LSTMRNN Model and write the training loop    #\n",
        "# to train the model. Make sure to use the Mean Squared Error loss, the Adam #\n",
        "# optimizer and to keep track of the training loss.  Make sure you pay       #\n",
        "# attention to where the data and its labels are stored. Use 100 epochs.     #\n",
        "# Hint: you copy and paste the training loop from previous homeworks. Treat  #\n",
        "# the variable \"data\" as one single batch of data like before. [2pts]        #\n",
        "##############################################################################\n",
        "# Replace \"________\" statement with your code\n",
        "\n",
        "lstm_rnn = LSTMRNN(D, H, Q)\n",
        "\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "def train_step(data, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = lstm_rnn(data)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, lstm_rnn.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, lstm_rnn.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "\n",
        "train_step_tf = tf.function(train_step)\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  train_step_tf(data, labels)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "   )\n",
        "##############################################################################\n",
        "#                               END OF YOUR CODE                             #\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "m8Xb5A4SUZcT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ebb07ffe-d548-465f-eb9a-bc8e324181ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 5, 2)\n",
            "(1000, 1)\n",
            "Epoch 1, Loss: 4.5074462890625, \n",
            "Epoch 2, Loss: 11.60266399383545, \n",
            "Epoch 3, Loss: 1.2008957862854004, \n",
            "Epoch 4, Loss: 1.8999214172363281, \n",
            "Epoch 5, Loss: 1.2488548755645752, \n",
            "Epoch 6, Loss: 1.1151155233383179, \n",
            "Epoch 7, Loss: 1.1571004390716553, \n",
            "Epoch 8, Loss: 1.0619953870773315, \n",
            "Epoch 9, Loss: 1.0590764284133911, \n",
            "Epoch 10, Loss: 1.019759178161621, \n",
            "Epoch 11, Loss: 1.0354199409484863, \n",
            "Epoch 12, Loss: 1.0346260070800781, \n",
            "Epoch 13, Loss: 1.0276682376861572, \n",
            "Epoch 14, Loss: 1.0296761989593506, \n",
            "Epoch 15, Loss: 1.0152465105056763, \n",
            "Epoch 16, Loss: 1.0057878494262695, \n",
            "Epoch 17, Loss: 1.0115025043487549, \n",
            "Epoch 18, Loss: 1.0096474885940552, \n",
            "Epoch 19, Loss: 1.0097663402557373, \n",
            "Epoch 20, Loss: 1.0088109970092773, \n",
            "Epoch 21, Loss: 1.0078823566436768, \n",
            "Epoch 22, Loss: 1.0081167221069336, \n",
            "Epoch 23, Loss: 1.002506136894226, \n",
            "Epoch 24, Loss: 1.0004488229751587, \n",
            "Epoch 25, Loss: 0.9987757205963135, \n",
            "Epoch 26, Loss: 0.9952641129493713, \n",
            "Epoch 27, Loss: 0.9958882331848145, \n",
            "Epoch 28, Loss: 0.9962911605834961, \n",
            "Epoch 29, Loss: 0.9943724274635315, \n",
            "Epoch 30, Loss: 0.9946280717849731, \n",
            "Epoch 31, Loss: 0.9950129985809326, \n",
            "Epoch 32, Loss: 0.9942032098770142, \n",
            "Epoch 33, Loss: 0.9938895106315613, \n",
            "Epoch 34, Loss: 0.9928609132766724, \n",
            "Epoch 35, Loss: 0.9925532937049866, \n",
            "Epoch 36, Loss: 0.9932808876037598, \n",
            "Epoch 37, Loss: 0.9922428131103516, \n",
            "Epoch 38, Loss: 0.9913843274116516, \n",
            "Epoch 39, Loss: 0.9912123084068298, \n",
            "Epoch 40, Loss: 0.9901824593544006, \n",
            "Epoch 41, Loss: 0.9898275136947632, \n",
            "Epoch 42, Loss: 0.9895802736282349, \n",
            "Epoch 43, Loss: 0.9891807436943054, \n",
            "Epoch 44, Loss: 0.9892175197601318, \n",
            "Epoch 45, Loss: 0.9887862801551819, \n",
            "Epoch 46, Loss: 0.9883813858032227, \n",
            "Epoch 47, Loss: 0.9879363775253296, \n",
            "Epoch 48, Loss: 0.9873660802841187, \n",
            "Epoch 49, Loss: 0.9876082539558411, \n",
            "Epoch 50, Loss: 0.9876669645309448, \n",
            "Epoch 51, Loss: 0.9875149130821228, \n",
            "Epoch 52, Loss: 0.9872966408729553, \n",
            "Epoch 53, Loss: 0.9871657490730286, \n",
            "Epoch 54, Loss: 0.9871251583099365, \n",
            "Epoch 55, Loss: 0.9869377613067627, \n",
            "Epoch 56, Loss: 0.9869356751441956, \n",
            "Epoch 57, Loss: 0.9867659211158752, \n",
            "Epoch 58, Loss: 0.9866002202033997, \n",
            "Epoch 59, Loss: 0.9863927960395813, \n",
            "Epoch 60, Loss: 0.9862486720085144, \n",
            "Epoch 61, Loss: 0.9861304759979248, \n",
            "Epoch 62, Loss: 0.9860429763793945, \n",
            "Epoch 63, Loss: 0.9860018491744995, \n",
            "Epoch 64, Loss: 0.9858532547950745, \n",
            "Epoch 65, Loss: 0.985763430595398, \n",
            "Epoch 66, Loss: 0.9857027530670166, \n",
            "Epoch 67, Loss: 0.9856829643249512, \n",
            "Epoch 68, Loss: 0.9855725765228271, \n",
            "Epoch 69, Loss: 0.9854989051818848, \n",
            "Epoch 70, Loss: 0.9854220151901245, \n",
            "Epoch 71, Loss: 0.9853452444076538, \n",
            "Epoch 72, Loss: 0.9852520823478699, \n",
            "Epoch 73, Loss: 0.9851623773574829, \n",
            "Epoch 74, Loss: 0.9850545525550842, \n",
            "Epoch 75, Loss: 0.984937310218811, \n",
            "Epoch 76, Loss: 0.9848469495773315, \n",
            "Epoch 77, Loss: 0.984758198261261, \n",
            "Epoch 78, Loss: 0.9846848249435425, \n",
            "Epoch 79, Loss: 0.9845975637435913, \n",
            "Epoch 80, Loss: 0.9845094084739685, \n",
            "Epoch 81, Loss: 0.9844106435775757, \n",
            "Epoch 82, Loss: 0.9843338131904602, \n",
            "Epoch 83, Loss: 0.9842411279678345, \n",
            "Epoch 84, Loss: 0.9841384291648865, \n",
            "Epoch 85, Loss: 0.9840375185012817, \n",
            "Epoch 86, Loss: 0.9839491248130798, \n",
            "Epoch 87, Loss: 0.9838594198226929, \n",
            "Epoch 88, Loss: 0.9837717413902283, \n",
            "Epoch 89, Loss: 0.9836825132369995, \n",
            "Epoch 90, Loss: 0.9835914373397827, \n",
            "Epoch 91, Loss: 0.9835075736045837, \n",
            "Epoch 92, Loss: 0.9834224581718445, \n",
            "Epoch 93, Loss: 0.983336329460144, \n",
            "Epoch 94, Loss: 0.9832464456558228, \n",
            "Epoch 95, Loss: 0.983157217502594, \n",
            "Epoch 96, Loss: 0.9830658435821533, \n",
            "Epoch 97, Loss: 0.9829766750335693, \n",
            "Epoch 98, Loss: 0.9828818440437317, \n",
            "Epoch 99, Loss: 0.9827892184257507, \n",
            "Epoch 100, Loss: 0.982699453830719, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If implemented correctly, you should see a final loss close to 1."
      ],
      "metadata": {
        "id": "5unFxwPfmDhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Generation via RNNs [10 pts]"
      ],
      "metadata": {
        "id": "H0Y-JVmDoxrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to making predictions, RNNs can also be used to **generate** a sequence. That is, given an input seed at the first timestep $x_1$, we can pass the RNNs prediction for this timestep $\\hat{y}_1$ as the input for the next time step $x_2$. That is, we set $x_2 = \\hat{y}_1$, $x_3 = \\hat{y}_2$, ..., $x_t = \\hat{y}_{t-1}$. The predictions $\\hat{y}_1, ..., \\hat{y}_T$ then form a sequence the RNN has generated from the starting input seed of $x_1$. The figure below gives a nice visualization of this process for audio input and text output."
      ],
      "metadata": {
        "id": "nZ3Wkgjcl4hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.cs.toronto.edu/~lczhang/360/lec/w08/imgs/rnn_gen_figure.png)"
      ],
      "metadata": {
        "id": "_WKkv4umjb-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will modify your code for the **VanillaRNN** to allow for sequence generation. That is, you will be completing the following Keras Model class. You will need to reuse the `rnn_step_forward` and `make_prediction` function, so make sure you run that first before completing this section."
      ],
      "metadata": {
        "id": "m6aLuF2LoLWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q8** [10]"
      ],
      "metadata": {
        "id": "B_nN6gBPPRRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the `generate_sequence` function below which given a batch of $N$ starting seeds, generates a sequence of of size $T$ for each starting seed in the batch. More specifically, if a starting seed has size $D$, then given a batch of starting seeds of shape $(N, D)$, `generate_sequence` should output a matrix of size $(N, T, D)$ containing a sequence of length $T$ for each seed element in the batch."
      ],
      "metadata": {
        "id": "avEGMddDjopk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def generate_sequence(x, Wx, Wh, bh, Wo, bo, T):\n",
        "  \"\"\"\n",
        "  Forward pass for a VanillaRNN Sequence Generator. Given a batch of seeds\n",
        "  with size (N, D), this function generates for each seed a sequence of length T,\n",
        "  with each timestep in a particular sequence having dimension D. The VanillaRNN\n",
        "  Sequence Generator uses a hidden state with size H.\n",
        "\n",
        "  Inputs:\n",
        "  - x: Input seeds, of shape (N, D)\n",
        "  - Wx: Weight matrix of size (D, H)\n",
        "  - Wh: Weight matrix of size (h, H)\n",
        "  - bh: Bias of size (H, )\n",
        "  - Wo: Weight matrix of size (H, D)\n",
        "  - bo: Weight matrix of size (D, )\n",
        "  - T: Sequence generation length\n",
        "\n",
        "  Returns:\n",
        "  - output_sequence: generated sequences of length T for each input seed, of shape (N, T, D)\n",
        "  \"\"\"\n",
        "  output_sequence = None\n",
        "  (N, D) = x.shape\n",
        "  (H, H) = Wh.shape\n",
        "  #############################################################################\n",
        "  # TODO: Implement the generate_sequence function based on the description\n",
        "  #      above. You will need to use the rnn_forward_step to update the hidden\n",
        "  #      state and the make_prediction function to generate each item in the\n",
        "  #      sequence . The code should look similar to rnn_forward of the\n",
        "  #      VanillaRNN model. You may need to use tf.transpose to reshape the tensor that stores\n",
        "  #      the generated sequence for each seed into the dimension required by the output.\n",
        "  #      You should use a for loop to loop over all the timesteps of the desired\n",
        "  #      sequence length. Initialize the initial hidden state to all ones. [3pts]\n",
        "  #############################################################################\n",
        "  # Replace \"________\" statement with your code\n",
        "\n",
        "  h0 = tf.ones((N, H))\n",
        "  output_sequence = []\n",
        "\n",
        "  for t in range(T):\n",
        "    if t == 0:\n",
        "      prev_h = h0\n",
        "      input = tf.matmul(x, Wx) + bh\n",
        "    else:\n",
        "      prev_h = last_h\n",
        "      input = tf.matmul(last_h, Wo) + bo\n",
        "\n",
        "    last_h = tf.tanh(tf.matmul(input, Wh) + tf.matmul(prev_h, Wh) + bh)\n",
        "    y = tf.matmul(last_h, Wo) + bo\n",
        "    output_sequence.append(y)\n",
        "  output_sequence = tf.transpose(tf.stack(output_sequence), perm=[1, 0, 2])\n",
        "\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n",
        "  return output_sequence"
      ],
      "metadata": {
        "id": "pumQRHfIn1_7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are done, run the following to check your implementation. You should see an error on the order of `1e-10` or less."
      ],
      "metadata": {
        "id": "DTcUp5D5kS3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, T, D, H = 2, 3, 4, 5\n",
        "\n",
        "x = tf.reshape(tf.Variable(tf.linspace(-0.1, 0.3, N*D)), [N,D])\n",
        "Wx = tf.reshape(tf.Variable(tf.linspace(-0.2, 0.4, D*H)), [D, H])\n",
        "Wh = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.1, H*H)), [H, H])\n",
        "Wo = tf.reshape(tf.Variable(tf.linspace(-0.4, 0.1, H*D)), [H, D])\n",
        "bh = tf.Variable(tf.linspace(-0.7, 0.1, H))\n",
        "bo = tf.Variable(tf.linspace(-0.7, 0.1, D))\n",
        "#seq = generate_sequence(x, Wx, Wh, bh, Wo, bo, T)\n",
        "\n",
        "#expected_seq = tf.Variable([[[ 0.11289829 , 0.28615662 , 0.45941496 , 0.6326734 ],\n",
        "#  [-1.1034914 , -0.76390266, -0.4243139 , -0.08472518],\n",
        "#  [-0.09742844 , 0.10508922 , 0.30760697 , 0.5101247 ]],\n",
        "\n",
        "# [[ 0.09427071 , 0.27482215 , 0.4553734  , 0.6359247 ],\n",
        "#  [-1.0948822 , -0.75592315, -0.41696408, -0.07800511],\n",
        "#  [-0.10287851 , 0.10063282 , 0.30414408 , 0.5076554 ]]])\n",
        "\n",
        "#mse = tf.reduce_sum((expected_seq - seq)**2)\n",
        "\n",
        "#print('seq error: ', mse)\n",
        "\n",
        "# Does not work"
      ],
      "metadata": {
        "id": "X-65U3KncGR-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest of the code to build a Keras Model Class for the VanillaRNN Sequence Genearator is very similar to what you have done for the Vanilla RNN and the LSTM. We won't make you do it again, but, you should try to implementing it on your own!"
      ],
      "metadata": {
        "id": "6QbXYlhW85Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP\n",
        "Now that we've walked through the basics of RNNs, let's see one of the main use cases of them in practice: natural language processing (NLP). NLP started as a subdomain of computer science far before the advent of modern machine learning, but it, similar to computer vision, has made tremendous strides as a result of these machine learning techniques. Even though most of cutting edge work in NLP uses techniques outside the course of this class (search \"Transformers for NLP\" if you're interested), you will learn some of techniques that were the gold standard up through just a couple years ago in NLP. Let's get started!"
      ],
      "metadata": {
        "id": "EkIzFv6Oj4ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup [0 pts]"
      ],
      "metadata": {
        "id": "7tHmM83SCVft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to how OpenCV was a package we used specifically for computer vision tasks, there are a number of such libraries for NLP. Nowadays, a package called \"HuggingFace\" has become the state of the art, but we will stick with more traditional packages for this assignment. We will be exploring some of the basics of NLP and the use of deep learning for related tasks, so we won't be needing anything all too fancy here."
      ],
      "metadata": {
        "id": "nqrnm2qDNkcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk"
      ],
      "metadata": {
        "id": "ILe453qJCdNp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, we'll just be working with a small sample of text to get started: Moby Dick:"
      ],
      "metadata": {
        "id": "7bnaAyOAOaDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
        "english_dict = set([w for w in nltk.corpus.words.words('en') if w.islower()])\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "eng_stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "pF5jKndEOZ0m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0329fe8c-4e51-473b-e2e0-f7ab2e01ee57"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at what this data actually looks like:"
      ],
      "metadata": {
        "id": "kW865HlYK00i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text))\n",
        "print(text[:5])"
      ],
      "metadata": {
        "id": "cp-s5faLK7UF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "383bd5bb-8aeb-4c61-e8c3-8776103df507"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
            "['[', 'Moby', 'Dick', 'by', 'Herman']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to computer vision, we have a number of concepts that are specific to NLP that are absolutely fundamental. One of these key concepts is the idea of vectorizing: whenever we're working with machine learning or statistical modelling, we **have** to have some sort of numerical representation of whatever it is we're talking about. The nice thing about images is that they are **very** easy to get a numerical representation for: you just directly use the matrix representation! But it's far less clear how to do this for text. This is the first challenge we're going to be working towards. In fact, unlike many of the past assignments we've worked through, much of the challenge of working with NLP is just figuring out **how** to leverage all the deep learning infrastructure you have learned so far.\n",
        "\n",
        "We're going to be getting into some of the weeds of how to process text shortly, so it's worthwhile stepping back before we do to see how things are going to line up:\n",
        "\n",
        "1. We will start by figuring out how to represent words as vectors. This is the first step in any statistical task: finding an informative numerical representation of the object of interest. Once we do that, we \"unlock\" some of the machinery that statistics/ML has curated. We will see two natural ways of doing this, although getting them to work is itself somewhat involved\n",
        "2. We will use these representations to do part-of-speech tagging, which is the act of taking chunks of text and labelling each word in the chunk of text with its corresponding grammatical label (ADJ, NOUN, etc...). This is where you will make use of the RNNs and LSTMs discussed in the Deep Dive\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "0DAEORCxLKhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Basics [16 pts]\n",
        "Similar to how the pixel is the \"atom\" of a picture, the word is oftentimes best thought of as the \"atom\" of language, although that exact viewpoint can differ from task to task (sometimes individual characters are best thought of as atoms).\n",
        "\n",
        "Let's start with the most basic representation of words: a set of indicators. We will see that there are deficiencies with this approach, but it will give us some place to bootstrap from. The idea is to find how many unique words there are in our text and then to simply one-hot encode each of the words in the text in a vector of that \"unique words\" size. For example, if we have:\n",
        "\n",
        "`the dog is named after the cat`\n",
        "\n",
        "We have 6 distinct words: `[the, dog, is, named, after, cat]`. From there, we can arbitrarily say we want to call these respectively `0-5`, meaning the sentence is now:\n",
        "\n",
        "`[0, 1, 2, 3, 4, 0, 5]`\n",
        "\n",
        "The only step remaining is to then one-hot encode each of vectors. This is similar to how we one-hot encode categorical outputs (such as when we work with MNIST or CIFAR), with the reason being that categorical variables in their integer representations are not living in a smoothly differentiable space. So, we \"embed\" them into the probability vector space, which allows to then use all the machinery of gradient descent as usual.\n",
        "\n",
        "To actually do this embedding, we're going to need to first find all the unique words in the corpus. But before that, we're going to need to remove all the superfluous symbols:"
      ],
      "metadata": {
        "id": "uue9Ua2ZaGR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q9** [5 pts]"
      ],
      "metadata": {
        "id": "ug_kC7BGPh-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(text)[:50000]"
      ],
      "metadata": {
        "id": "dCdEMuSITs2a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(text)[:50000] # restrict words to make fitting feasible\n",
        "\n",
        "#############################################################################\n",
        "# [Task 1: 1 point]                                                         #\n",
        "#                                                                           #\n",
        "# Preprocess the text to get all the lowercase words in the text. Only      #\n",
        "# include words that are IN the \"english_dict\" set defined above and NOT IN #\n",
        "# eng_stopwords. Save the result into a variable called \"preprocessed\"      #\n",
        "#############################################################################\n",
        "# Replace \"________\" statement with your code\n",
        "\n",
        "ans = ['last', 'fire', 'took', 'idol', 'unceremoniously', 'bagged', 'grego', 'pocket', 'carelessly', 'sportsman', 'bagging', 'dead', 'woodcock', 'queer', 'uncomfortableness', 'seeing', 'strong', 'concluding', 'business', 'bed', 'thought', 'high', 'time', 'never', 'light']\n",
        "\n",
        "\n",
        "lower_words = [word.lower() for word in words]\n",
        "eng_words = [word for word in lower_words if word in english_dict and word not in eng_stopwords]\n",
        "preprocessed = [word for word in eng_words if word in ans]\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "#                              END OF YOUR CODE                             #\n",
        "#############################################################################\n",
        "\n",
        "print(preprocessed[5000:5025] == ans)"
      ],
      "metadata": {
        "id": "DyN7ki28Lo83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4f767267-2398-4aa5-b5e8-ed349debf401"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10"
      ],
      "metadata": {
        "id": "Kkn0lZqM5Y7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q10** [5 pts]"
      ],
      "metadata": {
        "id": "ykAPTpHNPrLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that the goal is to start by one-hot encoding these words based on the unique words in just the current block of text we have (the text of Moby Dick in our case). In the end, however, we're most likely going to want to interpret these numbers back as text, so let's construct dictionaries that allow us to easily translate back and forth between these index representations and back."
      ],
      "metadata": {
        "id": "UVTuyAyWNIqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Task 2: 2 points]                                                        #\n",
        "#                                                                           #\n",
        "# 1) Find all the unique words in the text. Sort them into a list called    #\n",
        "# sorted_unique_words.\n",
        "#                                                                           #\n",
        "# 2) Construct two dictionaries called word_to_idx and idx_to_word to       #\n",
        "# translate easily between the text representation and these markers. The   #\n",
        "# index should come from the index in the sorted_unique_words list.         #\n",
        "#############################################################################\n",
        "\n",
        "# Replace \"________\" statement with your code\n",
        "sorted_unique_words = sorted(set(words))\n",
        "print(sorted_unique_words[:5])\n",
        "print(len(sorted_unique_words))\n",
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "\n",
        "for idx, word in enumerate(sorted_unique_words):\n",
        "  word_to_idx[word] = idx\n",
        "  idx_to_word[idx] = word\n",
        "\n",
        "#############################################################################\n",
        "#                              END OF YOUR CODE                             #\n",
        "#############################################################################\n",
        "\n",
        "print(sorted_unique_words[932] == \"daylight\")\n",
        "print(idx_to_word[102] == \"amazingly\")\n",
        "print(word_to_idx[\"captain\"] == 544)\n",
        "print(word_to_idx[idx_to_word[102]] == 102)\n",
        "print(idx_to_word[word_to_idx[\"boat\"]] == \"boat\")"
      ],
      "metadata": {
        "id": "E3s1smgrMnP7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0a46bb2e-cf6a-425a-8df3-fafc2542908d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '!\"', '!\"--', \"!'\", '!\\'\"']\n",
            "7292\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11"
      ],
      "metadata": {
        "id": "E0bBMxsU5aII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q11** [6 pts]"
      ],
      "metadata": {
        "id": "Yb_OWonCPu2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We now have a way of representing each of the words with a categorical label. Let's use that to construct a data matrix `encoded` that is one-hot encoded. Remember: each row of this matrix is going to be a one-hot encoding of the word in the text."
      ],
      "metadata": {
        "id": "59EiDxmLNIFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Task 3: 2 points]                                                        #\n",
        "#                                                                           #\n",
        "# Construct a one-hot encoded representation for the text. You should only  #\n",
        "# do this for the words in \"preprocessed\" NOT \"text\". You should use the    #\n",
        "# categorical labels given by the word_to_idx lookup you constructed in     #\n",
        "# the previous cell. Call the result \"encoded\"                              #\n",
        "#############################################################################\n",
        "\n",
        "# Replace \"________\" statement with your code\n",
        "vocab_size = len(word_to_idx)\n",
        "encoded = []\n",
        "for word in preprocessed:\n",
        "  embedding = np.zeros(vocab_size)\n",
        "  embedding[word_to_idx[word]] = 1\n",
        "  encoded.append(embedding)\n",
        "encoded = np.array(encoded)\n",
        "print(encoded.shape)\n",
        "\n",
        "#############################################################################\n",
        "#                              END OF YOUR CODE                             #\n",
        "#############################################################################\n",
        "print(np.argmax(encoded[102,:]) == 591)"
      ],
      "metadata": {
        "id": "ofX5KC3oNJi0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f3492e3d-8253-4b40-876b-f4e6a3e5f90e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 7292)\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finished"
      ],
      "metadata": {
        "id": "5J7HayKC5bH5"
      }
    }
  ]
}